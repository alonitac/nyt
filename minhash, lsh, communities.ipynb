{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find *stories* in the New York Time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lsh import minhash  # https://github.com/mattilyra/lsh\n",
    "import datetime\n",
    "from api_client import APIclient\n",
    "import pandas as pd\n",
    "import community\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ngram = 4\n",
    "bands = 20\n",
    "seeds = 100\n",
    "jaccard_min = 0.7\n",
    "jaccard_max = 0.95\n",
    "start_year = 1999\n",
    "end_year = 2020\n",
    "api_client = APIclient()\n",
    "\n",
    "hasher = minhash.MinHasher(seeds=seeds, char_ngram=char_ngram, hashbytes=4)\n",
    "\n",
    "def generate_shingles(text):\n",
    "    return set(text[head:head + char_ngram] for head in range(0, len(text) - char_ngram))\n",
    "\n",
    "def jaccard(set_a, set_b):\n",
    "    intersection = set_a & set_b\n",
    "    union = set_a | set_b\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def clean_text(df):\n",
    "    df['text'] = df['headline'].astype(str) + ' ' + df['snippet'].astype(str)\n",
    "    df['text'] = df['text'].apply(lambda x: x.encode('utf8'))\n",
    "    df = df[['_id', 'text']]\n",
    "    df = df.set_index('_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from Mongo, generate fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year):\n",
    "    res = api_client.aggregate(\n",
    "            [\n",
    "                {'$match': {'pub_date': {'$lte': datetime.datetime(year, 12, 31),\n",
    "                                         '$gte': datetime.datetime(year, 1, 1)}}},\n",
    "                {\n",
    "                    '$project':\n",
    "                        {\n",
    "                            'headline': '$headline.main',\n",
    "                            'snippet': '$snippet',\n",
    "                            # 'by': {'$substr': ['$byline.original', 3, -1]},\n",
    "                        }\n",
    "                }\n",
    "            ]\n",
    "    )\n",
    "    df = pd.DataFrame(list(res)).dropna()\n",
    "    df = clean_text(df)\n",
    "\n",
    "    df['fingerprint'] = df['text'].apply(lambda t: hasher.fingerprint(t))\n",
    "    df['fingerprint'].to_pickle('fingerprint/{}.pkl'.format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hash to buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [defaultdict(set) for _ in range(bands)]\n",
    "\n",
    "def bins_gen(fingerprint):\n",
    "    yield from enumerate(np.array_split(fingerprint, bands))\n",
    "\n",
    "def add_fingerprint(fingerprint, doc_id):\n",
    "    for bin_i, bucket in bins_gen(fingerprint):\n",
    "        bucket_id = hash(tuple(bucket))\n",
    "        bins[bin_i][bucket_id].add(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year):\n",
    "    df = pd.read_pickle('fingerprint/{}.pkl'.format(year))\n",
    "    for i in range(len(df)):\n",
    "        add_fingerprint(df.iloc[i], doc_id=df.index.values[i])\n",
    "    with open('bins/{}.pkl'.format(year), 'wb') as f:\n",
    "        pickle.dump(bins, f)\n",
    "    \n",
    "    del df\n",
    "    del bins\n",
    "    bins = [defaultdict(set) for _ in range(bands)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find candidate pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bin_i in range(bands):\n",
    "    b = defaultdict(set)\n",
    "    for year in range(start_year, end_year):\n",
    "        with open('bins/{}.pkl'.format(year), 'rb') as f:\n",
    "            bins = pickle.load(f)[bin_i]   \n",
    "            for bucket_id in bins:\n",
    "                b[bucket_id].update(bins[bucket_id])\n",
    "            del bins\n",
    "    print(bin_i)\n",
    "    with open('bins/bin_{}.pkl'.format(bin_i), 'wb') as f:\n",
    "        pickle.dump(b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pairs = set()\n",
    "for bin_i in range(bands):\n",
    "    with open('bins/bin_{}.pkl'.format(bin_i), 'rb') as f:\n",
    "        b = pickle.load(f) \n",
    "        for bucket_id in b:\n",
    "            if len(b[bucket_id]) > 1:\n",
    "                pairs = set(itertools.combinations(b[bucket_id], r=2))\n",
    "                candidate_pairs.update(pairs)\n",
    "        del b\n",
    "    print(bin_i)\n",
    "with open('candidate_pairs.pkl', 'wb') as f:\n",
    "        pickle.dump(candidate_pairs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Jaccard sim. Find communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_by_id(id):\n",
    "    res = api_client.aggregate(\n",
    "            [\n",
    "                {'$match': {'_id': id}},\n",
    "                {'$project': {'headline': '$headline.main', 'snippet': '$snippet'}}\n",
    "            ]\n",
    "        )\n",
    "    return clean_text(pd.DataFrame(list(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('candidate_pairs.pkl', 'rb') as f:\n",
    "        candidate_pairs = pickle.load(f)\n",
    "G = nx.Graph()\n",
    "for docid_a, docid_b in candidate_pairs:\n",
    "    shingles_a = generate_shingles(get_text_by_id(docid_a).iloc[0]['text'])\n",
    "    shingles_b = generate_shingles(get_text_by_id(docid_b).iloc[0]['text'])\n",
    "    jaccard_sim = jaccard(shingles_a, shingles_b)\n",
    "    if jaccard_min <= jaccard_sim <= jaccard_max:\n",
    "        G.add_edge(docid_a, docid_b, weight=jaccard_sim)\n",
    "        \n",
    "print('{} of actual pairs were found'.format(nx.number_of_edges(G)))\n",
    "\n",
    "partition = community.best_partition(G)\n",
    "comm = []\n",
    "for com in set(partition.values()):\n",
    "    comm.append([nodes for nodes in partition.keys() if partition[nodes] == com])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Stories' full data and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = []\n",
    "for c in comm:\n",
    "    res = api_client.aggregate(\n",
    "        [\n",
    "            {'$match': {'_id': {'$in': c}}},\n",
    "            {\n",
    "                '$project':\n",
    "                    {\n",
    "                        'pub_date': '$pub_date',\n",
    "                        'headline': '$headline.main',\n",
    "                        'url': '$web_url',\n",
    "                    }\n",
    "            },\n",
    "            {'$sort': {'pub_date': 1}}\n",
    "        ]\n",
    "    )\n",
    "    story = pd.DataFrame(list(res))\n",
    "\n",
    "    # if story has last less then 1 day, it isn't a story\n",
    "    if (story.iloc[-1]['pub_date'] - story.iloc[0]['pub_date']).days > 1:\n",
    "        stories.append(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, TapTool, OpenURL\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, save, output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_in_page = 40\n",
    "files_count = 0\n",
    "\n",
    "print('{} stories were found'.format(len(stories)))\n",
    "\n",
    "\n",
    "def save_stories(grid):\n",
    "    global files_count\n",
    "    output_file('stories_{}.html'.format(files_count))\n",
    "    files_count += 1\n",
    "    save(gridplot(grid))\n",
    "\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div id=\"Tooltip\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 16px; font-weight: bold;\">@headline</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 10px;\">Click to the article!</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "grid = []\n",
    "for i, story in enumerate(stories):\n",
    "    if i and not i % stories_in_page:\n",
    "        save_stories(grid)\n",
    "        grid = []\n",
    "    story['y'] = 1\n",
    "    p = figure(plot_height=100, plot_width=1200, title=story.iloc[0]['headline'],\n",
    "               x_axis_type='datetime', tools='tap', tooltips=TOOLTIPS)\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.yaxis.visible = False\n",
    "    source = ColumnDataSource(story)\n",
    "    p.square('pub_date', 'y', size=10, source=source, fill_alpha=0.5)\n",
    "    url = '@url'\n",
    "    taptool = p.select(type=TapTool)\n",
    "    taptool.callback = OpenURL(url=url)\n",
    "    grid.append([p])\n",
    "\n",
    "save_stories(grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
